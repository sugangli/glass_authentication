\section{Concluding Remarks}
\label{sec:conc}

We developed a system that uses head-movement patterns of users for direct
authentication to a wearable device. We developed a light-weight approach that 
infers head-movements of users in response to music and generates signatures
that are unique to every user.
Our technique specifically uses the dynamic time-warping (DTW) tool to 
generate a head-movement feature that contains the mean and standard 
deviation of the DTW score of each user, determined over a multiple-user data 
set. The matching is conducted based on a thresholding scheme.
%Experimental observations revealed that the head-movement signatures generated
%using the dynamic time-warping tool, in response to the same music track, were
%consistent in typical stationary environments such as user sitting or
%standing at one location while attempting to authenticate. We leveraged the
%consistency in the head-movement signatures to develop two classification
%algorithms, based on machine learning and adaptive thresholding, to
%efficiently and accurately label user signatures.
Through a 30 user based experiment based evaluation using accelerometer traces
collected using our data collection app on Google Glass, we observed that the
average true-acceptance rate of our approach is at 95.1$\%$ and the false
acceptance rates (FAR) at 3.9$\%$. 
From our evaluation over 30 subjects' data we observed that a 10 sec and $K=3$ 
yield the least $EER$ of 3.97$\%$. We observed that the FAR can be reduced by 
increasing the music cue duration or by using more samples for feature 
extraction (use Top $K$ samples) or by increasing the training data set. 
We implemented \systemname~on Google Glass and profiled the execution time of 
the key system modules and observed response time of about 4.47 sec for a 10 
sec music cue duration and reduces to 2.44 sec for a 5 sec music cue duration.
Our profiling indicated that the most compute intensive part of the app (of 
the order of few seconds) was the classifier that involved DTW computation. We 
believe that such high compute times can be reduced through optimized 
algorithms or through strategic techniques such as pipelining the data input 
and execution process. 
%We have also optimized our algorithms to 
%minimize the processing latencies and power consumption, making them suitable 
%for wearable devices.
The multi-user data sets were validated and verified during the
course of our evaluations and will be released for public use in near future.

\iffalse
In this paper, we present the design, implementation and evaluation
of Headbanger, a head gesture based authentication system.
Headbanger generates a signature from user's head-movements in
response to short duration of audio track with fast beats, and uses
this signature as the behavioral biometric for authentication.
Through extensive experiments on the prototype on Google Glass
involving ?? users, we demonstrated that Headbander can achieve **\%
accuracy and thus with proper audio stimuli, head-movement alone
will be reasonably sufficient to serve a effective behavioral
biometric for authentication. We believe this work will be the basis
for using audio catalyst to enrich more useful human contextual
applications.
\fi