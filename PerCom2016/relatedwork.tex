\section{Related Work}\label{sec:related}
%Headbanger is an authentication system which focused on musical head movement. Below, we review the related literature on mobile device authentication.


%\subsection{Wearable/mobile device based schemes}
Several studies have looked at head or eye movements for various purposes including user authentication.  
Harwin et al.~\cite{harwin1990analysis} are the first to use head gestures for human computer interaction.
Westeyn et al.~\cite{westeyn2004recognizing} used
eye-blinking pattern as a unique feature for
authentication. They achieved 82.02\% accuracy with 9 participants. %Compared
%to eye blinking pattern, head-movements can provide much more entropy,
%therefore can be considered as a more suitable biometric characteristic.
Rogers et al.~\cite{rogers2015approach} proposed to use unconscious blinking and head movement to identify a user from a group of users. In this method, users were asked to view rapidly changing pictures for 34 seconds before they can be identified.
Ishimaru et al.~\cite{ishimaru2014blink} comes close to our system
design; they proposed to combine the eye blinking frequency from the infrared
proximity sensor and head motion patterns from accelerometer sensor on Google
Glass to recognize activities (e.g., reading, talking, watching TV,
math problem solving). We note that recognizing activities is usually much easier than recognizing who is performing the activity, which is our objective in this study.
%The key difference of our approach from ~\cite{ishimaru2014blink} is
%that, their approach focused on common head-movement and eye-blink patterns
%when people employ the same activities such as reading, typing, etc.
%We carefully investigated the head-movements from human subjects and found
%that they are unique to each person.
%Our system also identified these head-movements with a higher accuracy (95$\%$
%versus 82$\%$ in ~\cite{ishimaru2014blink}).

There are also a number of physiological activity recognition studies
using computer vision methods~\cite{kjeldsen2001head,hernandezbioglass}.
While~\cite{kjeldsen2001head} primarily uses computer vision to detect
head gestures, BioGlass~\cite{hernandezbioglass}
combines Google Glass's accelerometers, gyroscope, and camera to
extract physiological signals of the wearer such as pulse
and respiratory rates. Camera processing on wearable devices, especially
Google Glass is compute intensive and has a high energy
budget~\cite{likamwa2014draining}.

Accelerometers have long been used to sense, detect and also recognize
movements in other parts of the body; for example, gait recognition requires
sensing in areas such as waist~\cite{ailisto2005identifying},
pocket~\cite{gafurov2007gait}, arm~\cite{okumura2006study,gafurov2008arm},
leg~\cite{gafurov2006biometric} and ankle~\cite{gafurov2011user}.
These techniques, though well known, may not be suitable for on wearable
devices due to complexity (computation and energy) in the machine learning
process.
%They are similar in that they collect the raw accelerometers data
%and apply signal processing and/or machine learning techniques  to perform
%authentication.

Hand gesture and touchscreen dynamics are often coupled for
authenticating to a (touchscreen) device. A number of contextual features
including biometrics~\cite{sae2012biometric} (e.g. finger length, hand size,
swipe/zoom speed, acceleration, click gap, contact size, pressure)
and behavioral feature (e.g. touch location,
swipe/zoom length,
swipe/zoom curvature, time, duration) have been exploited as
effective features for authentication
purpose~\cite{frank2013touchalytics,cai2013mobile,feng2014tips}.
While most of the techniques require users to explicitly conduct a
gesture following a specific pattern, TIPS~\cite{feng2014tips}
proposed a multi-stage filtering with dynamic template adaptation
strategy to perform the user authentication in uncontrolled
environments -- when a users naturally their phone.

%There is indeed a significant prior art in authentication system
%implementations using various techniques such as
%speech~\cite{reynolds2000speaker}, computer vision and image
%~\cite{bowyer2006survey}, graphical passwords~\cite{biddle2012graphical},
%gestures~\cite{sherman2014user}, biometric
%fingerprints~\cite{jain1997identity}.
%In this paper, we do acknowledge the viewpoint that our approach can also be
%used as a complementary scheme to most of the existing techniques in the
%authentication application space.

%It was extended by Kjeldsen~\cite{kjeldsen2001head} to more
%categories such as continuous control, spatial selection and
%symbolic selection

%Gafurov et al.~\cite{gafurov2006biometric} developed a wearable
%biometric gait authentication system. The sensor is attached to the
%lower leg to extract the gait patterns -- accelerations in three
%directions: vertical, forward-backward and sideways motion of the
%lower leg. A combination of these accelerations is used for
%authentication. ~\cite{gafurov2011user} ankle
%\subsection{Infrastructure based schemes}

%\subsection{Biometric-rich based schemes}
%Fingerprints

% are we the first one? the cloest work? how far we are going to reach out? categories in general: image? voice? touch?
% on glass; body movement;

%mobisys 2014 papers

%gesture based authentication

%Music based movements -- GaTech

%Japanese paper on google glass and eye-wink

%hardware fingerprinting -- check CCS 2014

%acc-based authentication
%http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=5634532&sortType%3Dasc_p_Sequence%26filter%3DAND%28p_IS_Number%3A5634461%29

%wearable security
%http://gaia.cs.uiuc.edu/papers/wss.pdf

%commercial solutions: Nymi -- eyewink blink pattern

%BioNym -- heartbeat 