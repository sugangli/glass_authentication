\section{Evaluation}\label{sec:results}

%In this section, we will present our evaluation results of the \systemname.
We conducted comprehensive evaluation of \systemname~through laboratory studies involving
human subjects -- our studies were approved by the Institutional Review Board (IRB) of our
institution. In the first phase of evaluation, we collected from volunteer participants accelerometer sensor
readings with Google Glass. We analyzed these traces offline on a PC.
Our evaluation in this phase is primarily aimed at determining the accuracy and robustness of \systemname.
In the second phase of evaluation, we implemented a \systemname~app and measured its processing latency. Our measurements suggest that \systemname~is indeed light-weight and can be executed on wearable devices such as Google Glass.

%based on profiling the execution time of each key
%functions associated with \systemname, on the Google Glass device.

\begin{figure}\centering
\includegraphics[width=.85\columnwidth]{figure/roc_dtw_cos_cor.eps}
\caption{\label{fig:roc_dtw_cos_cor} The impact of the distance computing algorithm (i.e., DTW, cosine distance, and Correlation). In this set of results, we varied the value of $n$ from 0.0 to 10.0 with an increment of 0.1, resulting in 100 thresholds in each case. We then plotted the TPR (y-axis) and FAR (x-axis) for each threshold. The results show that DTW delivers much better accuracies than the other two distance algorithms. As a result, in the remaining of the study, we will choose DTW for distance computing.}
\end{figure}

\begin{figure}\centering
\includegraphics[width=.85\columnwidth]{figure/roc_k_value.eps}
\caption{\label{fig:roc_k_value} The impact of different K values: $K=1$ and $K=3$. In this set of results, we varied the value of $n$ from 0.0 to 10.0 with an increment of 0.1, resulting in 100 thresholds in each case. We then plotted the TPR (y-axis) and FAR (x-axis) for each threshold. The results show that voting schemes ($K=3$) provide minor improvement on the performance. As a result, in the remaining of the study, we will choose $K=1$.}
\end{figure}



\begin{figure}[t]
\centering
\includegraphics [width=.85\columnwidth]{figure/roc_diff_size.eps}
\caption{The impact of training dataset size: 10, 20, and 30 samples. In this set of results, we varied the value of $n$ from 0.0 to 10.0 with an increment of 0.1, resulting in 100 thresholds in each case. We then plotted the TPR (y-axis) and FAR (x-axis) for each threshold. The results show that having 30 samples delivers the best performance without adding to the online authentication computing overhead.  As a result, in the remaining of the study, we will choose to have 30 samples.}
%The variable here is $n$. Each (TPR, FAR) data point in the curve corresponds to a different value of $n$}
\label{fig:roc_diff_size}
\end{figure}

%\begin{figure}[t]
%\centering
%\includegraphics [width=\columnwidth]{figure/top3_roc.eps}
%\caption{Evaluation of impact of music cue duration on TPR and FAR in Top-3
%voting scheme ($K = 3$). A 10 sec music snapshot is trimmed into music cues of
%10 sec, 6 sec and 5 sec correspondingly.The variable here is $n$. Each (TPR, FAR) data point in the curve corresponds to a different value of $n$}
%\label{fig:roc-top3}
%\end{figure}



\begin{figure}[t]
\centering
\includegraphics [width=.85\columnwidth]{figure/exp2_eer_vary_length.eps}
\caption{The EER value of \systemname~when users choose different music cue lengths (10 sec, 6 sec and 5
sec). We have an EER value of 6.65\% with a 5-second music cue, and 4.43\% with a 10-second music cue, which is better than results of many similar body-movement based authentication systems (such as those in~\cite{gafurov2006biometric,ahmed2015checksum,rogers2015approach}).}
\label{fig:eer-length}
\vspace{-1mm}
\end{figure}




\subsection{Authentication Accuracy of \systemname}

\subsubsection{Participants}
We had a total of 30 volunteer participants for this experiment, including a total of 19 males and 11 females.
The mean age of the participants was 29.7 years with a standard deviation
of 9.81 years. The youngest participant was 23 years old while the eldest was
54 years old.

\subsubsection{Procedure}
Our first experiment setup aimed at emulating the typical usage scenario
of \systemname~for authentication, where a user conducts head-movements in
response to a music cue played on the Google Glass device during a login
attempt.
%One of the participants executing the experiment trial wearing the
%Google Glass. %Figure~\ref{fig:setup} shows our experiment setup.
In this experiment, all participants were asked to wear a Google Glass
device. Participants who originally wore glasses (e.g.~corrective eyewear) were asked to remove their
glasses before conducting the experiment.
The trials were conducted in an academic environment and overseen by one of
our team members.
The Google Glass ran our data-collection app that played a piece of
music (music cue) for a specific duration, and recorded the accelerometer
sensor readings. %We conducted these trials for three duration values: 5s,
%6s and 10s, in order to yield a sufficient sample length. We also conducted a experiment with shorter music duration(2s and 3s), however, the accuracy drop significantly. %As we will show further, the accuracy of the system can
%significantly improve with the duration of the music cue; longer the duration
%better is the accuracy.
The sensor readings were recorded into a text file that was stored
in the Glass's memory and later transported to a PC for offline processing
through a Python script. The experiment was conducted in a well-lit indoor
academic laboratory environment.

During the course of a data collection session, the participants were allowed to take a
break or withdraw from data collection if they felt uncomfortable at any
point. %; for example, feeling
%dizzy after head-movement for a period of time, not being able to see clearly
%if near-sighted, etcetera.
The participants also could take a break for a minute after each trial.
Each trial lasted for the duration of the music cue played on the Glass, and
a total of 40 such trials were conducted for each of the 30 subjects.
The entire data collection effort lasted over a duration of 60 days, of which 15
subjects conducted their trials in a single sitting over a period of two
hours, while the rest of the trails were spread over 3 days on average per
subject.
%while, half of the trials of the rest 10 subjects were spread over 2 days.
%The experiment yielded three sets of data traces that each correspond to
%the three music cue durations we selected.

\subsubsection{Metrics}
We evaluate the accuracy of \systemname~using metrics that are commonly used in evaluating authentication systems, namely,
true positive rate TPR (percentage of true test samples that are
correctly accepted), false acceptance rate FAR (percentage of false test samples that are
mistakenly accepted), and true rejection rate TRR (percentage of true test
samples that are mistakenly rejected). These three metrics are, however, largely dependent on the choice of the classification threshold  parameter in \systemname --
a strict threshold in the classifier can lead to a high TRR value, while
overly relaxing the threshold can lead to a high FAR. Hence, in order to report the threshold-independent performance, we also consider equal error rate EER which is the percentage of errors when we have $FAR = TRR$.
%%and balanced accuracy BAC, where BAC = 1 - (FAR+FRR)/2.
%%We present the accuracy results through the receiver operating
%%characteristics (ROC) curves, TPR versus FAR, as shown in Figures~\ref{}
%%and~\ref{}.
%Figures~\ref{fig:roc-top1}, \ref{fig:roc-top3} and \ref{fig:eer-length} report
%the accuracy
%of \systemname~through the metrics stated above.
%%Figures~\ref{fig:roc-curves} (a)-(c) report the accuracy of
%%\systemname~through the metrics stated above.

%In general, we observe that even with a 5s music cue, \systemname~can achieve a TPR of ***\% at FAR of ***\% and EER of ***\% (when we have $K$=***, *** out of 40 trials from each user being used for training with  thresholding parameter value $n = 2.7$ with DTW). If users do not mind a cue duration of 10s,

%After evaluating the above metrics under various scenarios, the highlight is that \emph{our TPR can be as high as 95.1\% at FAR of 3.5\% and EER of 3.97\%} (when we have $K = 3$, and 30 (out of 40) trials from each user being used for training with a thresholding parameter value $n = 2.7$ with DTW). Next we discuss in more detail how the authentication accuracy results are impacted by various design parameters.

\subsubsection{Tuning Important System Parameters}
%Before presenting final authentication accuracy results, we first study how \systemname's performance is impacted by several important system parameters -- the choice of distance computing algorithm, $K$ value, and training dataset size -- and present the results  in Figures~\ref{fig:roc_dtw_cos_cor}-~\ref{fig:roc_diff_size}, respectively.
Figures~\ref{fig:roc_dtw_cos_cor}-\ref{fig:roc_diff_size} present the results on how the system's performance is impacted by several important system parameters, namely, the choice of sample distance computing algorithm, the number of best representative training samples used for classification ($K$), and the number of total training samples ($M$).
Based upon these results, we tune the parameter values to balance the tradeoff between authentication accuracy and data collection/computing overhead.

Recall that our classifier employs a distance threshold of ($\mu+n\sigma$), where $\mu$ and $\sigma$ are calculated from the training samples. For each parameter study, we varied the value of $n$ from 0.0 to 10.0 with an increment of 0.1, and had a total of 100 threshold values. We then plotted the TPR  on y-axis and FAR on x-axis for each threshold value,  resulting curves referred to as Receiver Operating Characteristics (ROC) curves. %ROC curves are usually used to describe the performance of an authentication system by varying the threshold~\cite{hanley1982meaning}.

Firstly, Figure~\ref{fig:roc_dtw_cos_cor} compares the performance of three distance computing algorithms: COS (cosine), COR (correlation) and DTW, assuming a single best representative training sample $K=1$, music cue duration of 10s, and 30 training samples $M=30$.  Among these three algorithms, DTW fares much better than the other two: its EER is 14.29\% smaller than that of COS distance, and 20.51\% smaller than that of COR distance. This is as expected because DTW is designed to match the waveform of two signals~\cite{dtw} and thus outputs more accurate distance score. As a result, in the remaining of this study, we will use DTW for evaluation. Even though DTW incurs more computation than the other two, our Google Glass implementation shows that through software optimization, the processing latency of DTW distance can be made very small (see Section~\ref{subsec:app}).


Secondly, Figure~\ref{fig:roc_k_value} compares the performance of two $K$ values: $K$=1 and $K$=3, assuming DTW distance, music cue duration of 10s, and 30 training samples. Recall that
our classifier compares the test sample against $K$ best representative training samples, generates $K$ independent classification results, and votes among them for the final authentication result. Hence, we expect that considering top 3 samples will be better than only considering the top 1 sample, as confirmed by the results shown in Figure~\ref{fig:roc_k_value}. However, we observe the improvement is very marginal: the EER when $K=3$ is only 0.5\% smaller than the EER when $K=1$. On the other hand, having $K=3$ incurs three times as much computation as having $K=1$. As a result, in the remaining of this study, we will use $K=1$ for evaluation.

Thirdly, Figure~\ref{fig:roc_diff_size} compares the performance of three training dataset sizes: M=10, 20, and 30 samples, assuming DTW distance, $K=1$, and music cue duration of 10s. We observe that the EER when $M=30$ is 1.79\% smaller than the EER when $M=20$ and 3.17\% smaller than that when $M=10$.  We emphasize that the value of $M$ has NO impact on the computing overhead in the online authentication phase because the classifier only compares the test sample against $K$ representative training samples. As a result, in the remaining of this study, we will use $M=30$ for the evaluation. Please note that we don't choose use a larger $M$ value because the benefit of having a larger $M$ diminishes quickly after having 30 samples.

 %***LS: need to extend to emphasize it has nothing to do with user experience since we can grow it after initialized data collection, hence we choose size 30 that gives the best result ***

%\paragraph{Impact of Training Dataset Size}
%%The accuracy of detecting and matching the head-movements to the user depend
%%upon the music cue duration, value of $K$, and number of samples used for
%%training.

%***YZ: we need to rewrite this section. In this section, we need to focus on how much improvement we have when we increase the data set size. Also, I think we are using way too many metrics. ***
%Recalling from Section~\ref{sec:design}, the input to the training phase
%is a set of temporal signals (samples with duration equal to the music cue
%duration), each corresponding to one trial of the head-movements from the
%user. Our evaluations so far considered a training set consisting of 30 samples.
%***I feel we have too many different metrics. It is confusing.*** In Figure~\ref{fig:eer-size}, we report the EER in \systemname~for three
%different training data set sizes; 10,20 and 30 samples.
%We can observe from Figure~\ref{fig:eer-size} that the EER holds an inverse
%relationship with the training set size. A larger training set minimizes the
%variance in mean and standard deviation computations, as the errors in their
%inconsistency are reduced by averaging the mean and standard deviation
%estimates over a larger set of data.
%On the other hand, a larger training set also implies a longer execution time
%of the training phase.
%However, in our system design, we posit that the training phase can be
%conducted
%offline on a more compute efficient device (smartphone, PC or server) and that
%the wearable device can pre-fetch the trained data (for example, an XML file),
%prior-to or during data collection phase, through a wireless link.

%are giving promising results for the response time sequence, hence we will evaluate these three algorithms in our end-to-end system. Also due to DTW is relatively more computing-intensive than the other two algorithm, we would like to see whether DTW  is worth of computing resource. In this experiment, we vary thresholding parameter value $n$ and fix the other parameters: 10 sec duration, $K = 1$, and  training data $size = 30$. We can observe from the ROC
%(receiver operating characteristic) curves in Figure~\ref{fig:roc_dtw_cos_cor} that DTW gives the best result among three algorithms, since its curve is the closest to the upper left corner. Our system preserves all characteristics of the data, which includes the response time to the music beat and the 3-axis accelerometer data. In terms of ~\cite{DTW}.

%\paragraph{Impact of $K$ value}
% We observe from the ROC curves in Figure~\ref{fig:roc-top1} and \ref{fig:roc-top3} that
%for both, $K=1$ and $K=3$, the TPR is close to 95\% while the FAR is slightly
%above 3\% for the 10 sec music duration. For $K = 1$ the FAR increases to
%about 7\% for the 5 sec and 6 sec cases, however, for $K = 3$ the FAR
%decreases to about 3\% and 4\%, respectively.
%We observe a similar trend with the EER as shown in
%Figure\ref{fig:eer-length}, where improvements of 0.5 - 1 \%
%can be achieved by choosing $K = 3$ over $K = 1$. ***YZ: if the improvement is less than 1\%, then K=1 is sufficient. Then for the music cue, please just use K=1***
%This indicates that the accuracy in \systemname~can improve with a larger
%value of $K$. However, the improvement in accuracy through redundancy in the
%training set trades off with the increased execution time as the
%matching requires at least $K - 1$  extra DTW computations as opposed to only
%1 for a top-1 scheme. As we will show ahead, DTW computations incur heavy CPU
%budget on the wearable device.
%***YZ: I will re-write this paragraph after you reorganize the results ***
%Finally, we note that in Section~\ref{subsec:app}, we have implemented a Headbanger app for Google Glass, and measured the computing latency of the app using the chosen parameter values. We show that the resulting computing latencies are very reasonable.


\subsubsection{Authentication Accuracy Results}
%We measure the DTW computation on Google Glass to be the most
%computationally expensive operation of all the software modules in
%\systemname.
%\paragraph{Impact of music cue duration}

%***YZ: I need to rewrite this paragraph. The tone needs to be changed. What is the improvement? ***

After tuning system parameters to balance accuracy and computing overhead, we next calculate the EER value of the resulting~\systemname~system when users choose different music cue durations and present the results in Figure~\ref{fig:eer-length}. As soon as a user starts the authentication procedure, how long the user must wait before she receives the authentication result is an important quality-of-service metric, which we refer to as \emph{authentication latency}.  Authentication latency consists of two parts: data input latency and data processing latency, wherein the former is the time a user spends on listening to the music cue and making head movements while the latter is the time \systemname~spends on computing the authentication result. Between these two parts, data input latency is by far the bottleneck as the data processing latency can be easily reduced (by better algorithms and/or faster hardware) and/or hidden (by pipelining computing with data collection). Unfortunately, data input latency is hard to be reduced or hidden by the improvement in software or hardware. Recognizing that different users can tolerate different latencies and desire different levels of authentication accuracy, \systemname~allows the users to choose the music cue duration (which has the same length as the data input latency).

From Figure~\ref{fig:eer-length}, we observe that the EER value of \systemname~is as low as 6.65\% with a 5-second music cue, and 4.43\% with a 10-second music cue. We take the viewpoint that such error rate is rather sufficient for personal head-worn devices that are not used in hostile environments. Also, the data input latency of 5-10 seconds is comparable with similar authentication systems. For example, a gait-based authentication system~\cite{gafurov2006biometric} delivers an EER of 7.3\% after observing the user for 20 meters;
an arm-swing-based authentication system~\cite{ahmed2015checksum} delivers a TPR of 91.6\% and  a FAR of 5.5\%  with 5.7 seconds of processing latency; an eye-blinking-based authentication system~\cite{rogers2015approach} delivers the BAC=(TPR + FAR)/2 of 94.4\%, with a processing latency of 32 seconds.
%
%In general, we observe from the results that the FAR can
%be decreased by increasing the music cue duration.
%We can observe (from Figures~\ref{fig:roc-top1}, \ref{fig:roc-top3} and
%\ref{fig:eer-length}) that the improvement is less significant when the music
%cue duration is increased from 5 sec to 6 sec, however, the improvement is
%more
%significant when the music cue duration is increased to 10 sec.
%In \systemname~the data collection phase for the authentication system
%(sampling accelerometer sensor readings) is executed in parallel with the
%music cue. The data processing phase involving the filtering, classification
%and matching is executed only at the end of the music cue and in the same
%order.
%Hence, execution time of the app will be independent of this duration.
%Finally, we note that, the data input duration of 5-10 sec for authentication is on par with those
%of PIN-based and pattern-based system~\cite{von2013patterns}, where the former takes 1.5 sec and the later takes 3.1 sec on average. ***YZ: 5-10 seconds are much larger than 1.5/3.1 seconds!!*** \textbf{Yeah. Also, such discussion belongs to Discussion and not results.}
%We also note that, since the authentication process is initiated only at the
%end of the music cue, execution time of the app will be independent
%of this duration.

%\begin{figure}[t]
%\centering
%\includegraphics [width=\columnwidth]{figure/exp2_vary_size.eps}
%\caption{Comparison of EER for different training sample sizes (30, 20 and 10)
%with fixed n value of 2.7}
%\label{fig:eer-size}
%\end{figure}





%\paragraph{Impact of Sampling Rate}
%***YZ: this can be removed ***
%Due to DTW is resource-consuming, one direct way to reduce the workload of our algorithm is to decrease the sampling rate of the accelarometer and remain the sampling time. Based on Nyquist–Shannon sampling theorem, since the high cut of our filter is 10 Hz, the minimum sampling rate should be 20 Hz. Also, the highest sampling rate and the default sampling rate of our platform are 200 Hz and 50 Hz accordingly. Thus, we choose these three value to evaluate the impact of sampling rate.  In Figure~\ref{fig:roc_dtw_diff_freq}, we observe that 200 Hz provides the best performance among three value, while 50 Hz and 20 Hz are giving closed results. The EER of 20 Hz is **, comparing with  ** of that of 200 Hz. Since the decrement of EER is insignificant and we can achieve 10 times speedup in DTW computing,  20 Hz will be a ideal value of system implementation.
\begin{figure}
\vspace{-1mm}
\centering

\includegraphics[width = .85\columnwidth]{figure/imitation_subject_movement.eps}

\caption{\label{fig:imitation_movement} Pictorial description of the nodding pattern employed by each target. Target (a) only moved his head in the vertical direction, and his nodding immediately follows each music beat (on-beat); target (b) only moved his head in the vertical direction, but there is often a delay between his nodding and the music beat (off-beat); target (c) occasionally combined shaking with nodding, and on-beat. As a result, the nodding patterns in (b) and (c) are slightly more complex than that in (a). }
\vspace{-1mm}
\end{figure}

\input{eval2} 