\section{Related Work}\label{sec:related}
Headbanger is a 3D behavioral biometric based
activity recognition system, which focuses on head movement in this paper. 
To our best knowledge, it is the first wearable authentication system that only relies on
accelerometer signatures from human \emph{head} motion. Below, we briefly review the related literature on mobile device authentication.

%in wearable/mobile device based schemes, infrastructure based
%schemes, and biometric-rich based schemes.

%\subsection{Wearable/mobile device based schemes}
Harwin et al.~\cite{harwin1990analysis} was considered the first that
proposed to use head gestures by combining pointing and movements
for human computer interaction. The closest work to ours is
the one in~\cite{westeyn2004recognizing}, in which eye
blinking pattern was used as a unique feature for
authentication. They achieved 82.02\% accuracy with 9 participants. Compared to eye blinking pattern, head-movements provide much more entropy, therefore a more suitable biometric characteristic. 
Ishimaru et al.~\cite{ishimaru2014blink} proposed to combine the eye blinking frequency from the infrared proximity sensor and head motion patterns from accelerometer sensor on Google Glass
to recognize activities (e.g., reading, talking, watching TV,
math problem solving), and achieved 82\% recognition
accuracy. While their approach looked at common patterns when people employ the same activities, ours took a deeper look at the head-movements and found that everyone's head-movements are unique. There are also a number of head
motion based activity recognition studies using computer vision, such
as the one in~\cite{kjeldsen2001head}. BioGlass~\cite{hernandezbioglass}
combines Google Glass's accelerometers, gyroscope, and camera to
extract physiological signals of the wearer such as pulse
and respiratory rates.

Accelerometers have also been used to measure movements in other part of the
body for gait-based authentication purpose, such as
waist~\cite{ailisto2005identifying}, pocket~\cite{gafurov2007gait},
arm~\cite{okumura2006study,gafurov2008arm},
leg~\cite{gafurov2006biometric} and ankle~\cite{gafurov2011user}.
They are similar in that they collect the raw accelerometers data
and apply signal processing and/or machine learning techniques  to perform
authentication.

Hand gesture and touchscreen dynamics are often coupled for
authenticating that device. A number of contextual features
including biometrics (e.g. finger length, hand size,
swipe/zoom speed, acceleration, click gap, contact size, pressure)
and behavioral feature (e.g. touch location, swipe/zoom length,
swipe/zoom curvature, time, duration) have been exploited as
effective features for authentication purpose such as demonstrated
in~\cite{sae2012biometric,frank2013touchalytics,cai2013mobile,feng2014tips}.
While most of the techniques require users to explicitly conduct a
gesture following a specific pattern, TIPS~\cite{feng2014tips}
proposed a multi-stage filtering with dynamic template adaptation
strategy to perform the user authentication in a uncontrolled
environments -- as user naturally use the phone. There are a number of
authentication schemes using techniques such as
speech~\cite{reynolds2000speaker}, computer vision and image
~\cite{bowyer2006survey}, graphical
password~\cite{biddle2012graphical,sherman2014user}, biometric
fingerprints~\cite{jain1997identity}. 


Finally, we take the viewpoint that our approach can be used as a complementary scheme to most
of the existing techniques.

%It was extended by Kjeldsen~\cite{kjeldsen2001head} to more
%categories such as continuous control, spatial selection and
%symbolic selection

%Gafurov et al.~\cite{gafurov2006biometric} developed a wearable
%biometric gait authentication system. The sensor is attached to the
%lower leg to extract the gait patterns -- accelerations in three
%directions: vertical, forward-backward and sideways motion of the
%lower leg. A combination of these accelerations is used for
%authentication. ~\cite{gafurov2011user} ankle
%\subsection{Infrastructure based schemes}

%\subsection{Biometric-rich based schemes}
%Fingerprints

% are we the first one? the cloest work? how far we are going to reach out? categories in general: image? voice? touch?
% on glass; body movement;

%mobisys 2014 papers

%gesture based authentication

%Music based movements -- GaTech

%Japanese paper on google glass and eye-wink

%hardware fingerprinting -- check CCS 2014

%acc-based authentication
%http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=5634532&sortType%3Dasc_p_Sequence%26filter%3DAND%28p_IS_Number%3A5634461%29

%wearable security
%http://gaia.cs.uiuc.edu/papers/wss.pdf

%commercial solutions: Nymi -- eyewink blink pattern

%BioNym -- heartbeat
